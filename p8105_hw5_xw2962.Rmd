---
title: "p8105_hw5_xw2962"
author: "Xiaoyu Wu"
date: "2023-11-04"
output: github_document
---
 
```{r,message=FALSE}
library(dplyr)
library(tidyverse)
library(readr)
library(rvest)
library(purrr)
library(broom)
library(tidyr)
set.seed(1)
```

## Problem One 

#### Import Raw Data 
```{r,message=FALSE}
homicide_df= 
  read_csv("./data/homicide-data.csv")|>
  janitor::clean_names() 
```

#### Describe the Raw Data 
The dataset homicide_df has `r ncol(homicide_df)` variables: `r colnames(homicide_df)`. And in this dataset, there are `r nrow(homicide_df)` rows. And there are `r ncol(homicide_df)` columns. Here, we consider variables: reported_date, victim_race, victim_age, victim_sex, city, state, lat, lon and disposition important for analysis. 

#### Summarize within Cities 
```{r}
city_summary=
  homicide_df |> 
  mutate(city_state = paste(city, state, sep = ", "),
# create a city_state variable
         unsolved = disposition %in% c('Closed without arrest', 'Open/No arrest')) |> 
  group_by(city) |> 
  summarise(
    total_homicides = n(),
# summarize within cities to obtain the total number of homicides
    unsolved_homicides = sum(unsolved)
  ) |> 
# summarize within cities to obtain the number of unsolved homicides
  ungroup() 
```
#### Estimate the Proportion of Homicides that are Unsolved for Baltimore 
```{r}
baltimore_df <- homicide_df  |>
  mutate(city_state = paste(city, state, sep = ", ")) |>
  filter(city_state == "Baltimore, MD")
# filter for Baltimore, MD
prop_test_result = prop.test(x = sum(baltimore_df$disposition %in% c("Closed without arrest", "Open/No arrest")),
                             n = nrow(baltimore_df),
                             correct = FALSE) 
# perform the proportion test for Baltimore
tidy_result = broom::tidy(prop_test_result)
# save the output and apply broom::tidy to get a tidy dataframe
estimated_proportion = tidy_result$estimate
conf_low = tidy_result$conf.low
conf_high = tidy_result$conf.high
# pull the estimated proportion and confidence intervals from the resulting tidy dataframe
```
#### Run prop.test for each City
```{r}
city_prop_test_results = city_summary  |> 
  mutate(
    prop_test_result = map2(unsolved_homicides, total_homicides, ~prop.test(x = .x, n = .y)),
# run prop.test for each of the cities in your dataset
    tidy_result = map(prop_test_result, broom::tidy)
  ) |>
  select(city, tidy_result) |>
  unnest(tidy_result) |>
  select(-statistic, -p.value,-parameter, -method, -alternative) 
# create a tidy dataframe with estimated proportions and CIs for each city
print(city_prop_test_results)
# print the resulted dataframe 
```
#### Create a Plot that Shows the Estimates and CIs for each City 
```{r}
ggplot(city_prop_test_results, aes(x = reorder(city, estimate), y = estimate)) +
  geom_point() +
# create a point plot that shows the estimates and CIs for each city and organize cities according to the proportion of unsolved homicides.
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = 0.2) +
# check out geom_errorbar
  coord_flip() +  
# flips the axes to make it easier to read city names
  labs(x = "City", y = "Proportion of Unsolved Homicides",
       title = "Proportion of Unsolved Homicides with Confidence Intervals",
       subtitle = "For major cities as reported by The Washington Post") +
# add title, subtitle and axis labels
  theme_minimal()
# edit the theme 
```

## Problem Two 

#### Start with a Dataframe Containing all File Names
```{r}
data_path = "./data/problem_2_data/"
file_names = list.files(path = data_path, full.names = TRUE)
# create a dataframe of all file names
```
#### Iterate over File Names and Read in Data for each Subject
```{r, message=FALSE}
files_df = data.frame(file_path = file_names) |>
# create a dataframe with file names and extract relevant parts: subject ID and arm 
  mutate(file_name = basename(file_path),
         subject_id = sub(".*_(\\d+).csv", "\\1", file_name), 
# extract subject ID
         arm = substr(file_name, 1, 3)) 
# determine the arm

read_data = function(file_path) {
  read_csv(file_path) |>
    gather(key = "week", value = "observation") 
}
# iterate over file names and read in data for each subject by creating a function 
files_df = files_df |>
  mutate(data = map(file_path, read_data))
# map all files to the function created 
``` 
#### Tidy the Result
```{r, message=FALSE}
tidy_df = files_df %>%
  select(-file_path, -file_name) %>%
  unnest(data) |> 
  mutate(week = as.numeric(gsub("week_", "", week)))
# tidy the resulting dataset 
```
#### Make a Spaghetti Plot
```{r}
spaghetti_plot = ggplot(tidy_df, aes(x = week, y = observation, group = subject_id)) +
  geom_line(aes(color = arm), alpha = 0.4) +
# make a Spaghetti Plot and set the transparency level to 0.4 
  facet_wrap(~ arm, scales = 'free_y') +  
# Separating the groups into different arms 
  theme_minimal() +
  labs(title = "Observations Over Time by Subject and Group",
       x = "Week",
       y = "Observation",
       color = "Group") +
# add title and axis labels 
  theme(legend.position = "bottom")
print(spaghetti_plot)
# print result 
```
#### Comments on Differences between Groups
Unlike the control group, the experimental group's subjects have observation values increasing overtime and have a clear increasing trend. The observation values for control group fluctuate between -2 and 4. Whereas, the observation values for experimental group increase from 0 to around 8. 